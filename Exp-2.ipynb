{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. WAP to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "#### This code implements a Multi-Layer Perceptron (MLP) to learn the XOR function using NumPy. Here's a breakdown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm XOR NAND\n",
    "\n",
    "---\n",
    "### 1. Class Definition (MLP)\n",
    "The MLP class encapsulates all functionality required for training and evaluating the neural network.\n",
    "\n",
    "#### Structure:\n",
    "- Input layer: 2 neurons (for the two inputs in XOR)\n",
    "- Hidden layer: 2 neurons (to capture non-linearity)\n",
    "- Output layer: 1 neuron (binary classification)\n",
    "- Activation Functions:\n",
    "Sigmoid function (sigmoid),Sigmoid derivative (sigmoid_derivative):\n",
    "\n",
    "### 2. Training (train)\n",
    "The train method performs forward and backward propagation:\n",
    "\n",
    "#### Forward Pass:\n",
    "- Computes activations for the hidden and output layers.\n",
    "- Error Calculation:\n",
    "- Uses Mean Absolute Error (MAE) as a loss function.\n",
    "#### Backpropagation:\n",
    "- Computes gradients using sigmoid derivative.\n",
    "- Updates weights and biases using Gradient Descent.\n",
    "- Prints the loss every 1000 epochs.\n",
    "### 3. Prediction (predict)\n",
    "- Performs forward propagation.\n",
    "\n",
    "### 4. Evaluation (evaluate)\n",
    "Uses accuracy score and confusion matrix to assess performance.\n",
    "\n",
    "### 5. Visualization\n",
    "- Loss Curve (plot_loss_curve)\n",
    "- Decision Boundary (plot_decision_boundary)\n",
    "\n",
    "###  6. Running the Model\n",
    "XOR dataset is defined: [[0,0], [0,1], [1,0], [1,1]] with labels [[0], [1], [1], [0]].\n",
    "Trains the model using train().\n",
    "Evaluates performance using evaluate().\n",
    "Plots:\n",
    "Loss curve (plot_loss_curve).\n",
    "Decision boundary (plot_decision_boundary).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: -2\n",
      "Epoch 1000, Error: 2\n",
      "Epoch 2000, Error: 2\n",
      "Epoch 3000, Error: 2\n",
      "Epoch 4000, Error: 2\n",
      "Epoch 5000, Error: 2\n",
      "Epoch 6000, Error: 2\n",
      "Epoch 7000, Error: 2\n",
      "Epoch 8000, Error: 2\n",
      "Epoch 9000, Error: 2\n",
      "Predicted Outputs:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step activation function\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(1)\n",
    "w1 = np.random.randn(2, 2)  # Weights for input to hidden\n",
    "b1 = np.random.randn(1, 2)  # Bias for hidden layer\n",
    "w2 = np.random.randn(2, 1)  # Weights for hidden to output\n",
    "b2 = np.random.randn(1, 1)  # Bias for output layer\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "def train(X, Y, epochs=10000):\n",
    "    global w1, b1, w2, b2\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        hidden_input = np.dot(X, w1) + b1\n",
    "        hidden_output = step_function(hidden_input)\n",
    "        final_input = np.dot(hidden_output, w2) + b2\n",
    "        final_output = step_function(final_input)\n",
    "        \n",
    "        # Compute error\n",
    "        error = Y - final_output\n",
    "        \n",
    "        # Backpropagation (manual update using XOR logic)\n",
    "        w2 += lr * np.dot(hidden_output.T, error)\n",
    "        b2 += lr * np.sum(error, axis=0, keepdims=True)\n",
    "        hidden_error = np.dot(error, w2.T) * (hidden_output * (1 - hidden_output))  # Derivative\n",
    "        w1 += lr * np.dot(X.T, hidden_error)\n",
    "        b1 += lr * np.sum(hidden_error, axis=0, keepdims=True)\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}, Error: {np.sum(error)}')\n",
    "\n",
    "# Train the MLP\n",
    "train(X, Y)\n",
    "\n",
    "# Testing\n",
    "hidden_input = np.dot(X, w1) + b1\n",
    "hidden_output = step_function(hidden_input)\n",
    "final_input = np.dot(hidden_output, w2) + b2\n",
    "final_output = step_function(final_input)\n",
    "\n",
    "print(\"Predicted Outputs:\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
